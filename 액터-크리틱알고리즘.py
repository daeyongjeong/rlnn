#  ì•¡í„°-í¬ë¦¬í‹±ì•Œê³ ë¦¬ì¦˜
np.random.seed(0)

# í™˜ê²½, ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”
env = Environment()
agent = Agent()
gamma = 0.9

# p(ğ‘ ,ğ‘), ğ‘‰(ğ‘ )â†ì„ì˜ì˜ ê°’
V = np.random.rand(env.reward.shape[0], env.reward.shape[1])
policy = np.random.rand(env.reward.shape[0], env.reward.shape[1],len(agent.action))
# í™•ë¥ ì˜ í•©ì´ 1ì´ ë˜ë„ë¡ ë³€í™˜
for i in range(env.reward.shape[0]):
    for j in range(env.reward.shape[1]):
        policy[i,j,:] = policy[i,j,:] /np.sum(policy[i,j,:])

max_episode = 10000
max_step = 100

print("start Actor-Critic")
alpha = 0.1

# ê° ì—í”¼ì†Œë“œì— ëŒ€í•´ ë°˜ë³µ :
for epi in tqdm(range(max_episode)):
    # S ë¥¼ ì´ˆê¸°í™”
    i = 0
    j = 0
    agent.set_pos([i,j])

    # ì—í”¼ì†Œë“œì˜ ê° ìŠ¤í…ì— ëŒ€í•´ ë°˜ë³µ :
    for k in range(max_step):
        
        # Actor : p(ğ‘ ,ğ‘)ë¡œ ë¶€í„° aë¥¼ ì„ íƒ ( ì˜ˆ :Gibbs softmax method)
        pos = agent.get_pos()
        
        # Gibbs softmax method ë¡œ ì„ íƒë  í™•ë¥ ì„ ì¡°ì •
        pr = np.zeros(4)
        for i in range(len(agent.action)):
            pr[i] = np.exp(policy[pos[0],pos[1],i])/np.sum(np.exp(policy[pos[0],pos[1],:]))
                
        # í–‰ë™ ì„ íƒ
        action =  np.random.choice(range(0,len(agent.action)), p=pr)            
        
        # í–‰ë™ aë¥¼ ì·¨í•œ í›„ ë³´ìƒ rê³¼ ë‹¤ìŒ ìƒíƒœ s'ë¥¼ ê´€ì¸¡
        observation, reward, done = env.move(agent, action)
        
        # Critic í•™ìŠµ
        # Î´t=r(t+1)+Î³V(S(t+1) )-V(St)
        td_error = reward + gamma * V[observation[0],observation[1]] - V[pos[0],pos[1]]
        V[pos[0],pos[1]] += alpha * td_error

        # Actor í•™ìŠµ :
        # p(st,at)=p(st,at)-Î²Î´_t
        policy[pos[0],pos[1],action] += td_error * 0.01
        
        # í™•ë¥ ì— ìŒìˆ˜ê°€ ìˆì„ê²½ìš° ì „ë¶€ ì–‘ìˆ˜ê°€ ë˜ë„ë¡ ë³´ì •
        if np.min(policy[pos[0],pos[1],:]) < 0:
            policy[pos[0],pos[1],:] -= np.min(policy[pos[0],pos[1],:])
        for i in range(env.reward.shape[0]):
            for j in range(env.reward.shape[1]):
                policy[i,j,:] = policy[i,j,:] /np.sum(policy[i,j,:])
        
        # sê°€ ë§ˆì§€ë§‰ ìƒíƒœë¼ë©´ ì¢…ë£Œ
        if done == True:
            break

# í•™ìŠµëœ ì •ì±…ì—ì„œ ìµœì  í–‰ë™ ì¶”ì¶œ
optimal_policy = np.zeros((env.reward.shape[0],env.reward.shape[1]))
for i in range(env.reward.shape[0]):
    for j in range(env.reward.shape[1]):
        optimal_policy[i,j] = np.argmax(policy[i,j,:])
        
print("Actor - Critic : V(s)")
show_v_table(np.round(V,2),env)
print("Actor - Critic : policy(s,a)")
show_q_table(np.round(policy,2),env)
print("Actor - Critic : optimal policy")
show_policy(optimal_policy,env)   