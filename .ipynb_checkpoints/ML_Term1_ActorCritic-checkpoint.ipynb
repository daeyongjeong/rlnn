{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tic Tac Toe 환경 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "\n",
    "    def __init__(self):\n",
    "        # 보드는 0으로 초기화된 9개의 배열로 준비\n",
    "        # 게임종료 : done = True\n",
    "        self.board_a = np.zeros(16)\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        self.winner = 0\n",
    "        self.print = False\n",
    "\n",
    "    def move(self, p1, p2, player):\n",
    "        # 각 플레이어가 선택한 행동을 표시 하고 게임 상태(진행 또는 종료)를 판단\n",
    "        # p1 = 1, p2 = -1로 정의\n",
    "        # 각 플레이어는 행동을 선택하는 select_action 메서드를 가짐\n",
    "        if player == 1:\n",
    "            pos = p1.select_action(env, player)\n",
    "        else:\n",
    "            pos = p2.select_action(env, player)\n",
    "\n",
    "        # 보드에 플레이어의 선택을 표시\n",
    "        self.board_a[pos] = player\n",
    "        if self.print:\n",
    "            print(player)\n",
    "            self.print_board()\n",
    "        # 게임이 종료상태인지 아닌지를 판단\n",
    "        self.end_check(player)\n",
    "\n",
    "        return self.reward, self.done\n",
    "\n",
    "    # 현재 보드 상태에서 가능한 행동(둘 수 있는 장소)을 탐색하고 리스트로 반환\n",
    "    def get_action(self):\n",
    "        observation = []\n",
    "        for i in range(16):\n",
    "            if self.board_a[i] == 0:\n",
    "                observation.append(i)\n",
    "        return observation\n",
    "\n",
    "    # 게임이 종료(승패 또는 비김)됐는지 판단\n",
    "    def end_check(self, player):\n",
    "        # 0 1 2\n",
    "        # 3 4 5\n",
    "        # 6 7 8\n",
    "        # 승패 조건은 가로, 세로, 대각선 이 -1 이나 1 로 동일할 때\n",
    "        end_condition = (\n",
    "            (0, 1, 2, 3),\n",
    "            (4, 5, 6, 7),\n",
    "            (8, 9, 10, 11),\n",
    "            (12, 13, 14, 15),\n",
    "            (0, 4, 8, 12),\n",
    "            (1, 5, 9, 13),\n",
    "            (2, 6, 10, 14),\n",
    "            (3, 7, 11, 15),\n",
    "            (0, 5, 10, 15),\n",
    "            (3, 6, 9, 12),\n",
    "        )\n",
    "        for line in end_condition:\n",
    "            if (\n",
    "                self.board_a[line[0]] == self.board_a[line[1]]\n",
    "                and self.board_a[line[1]] == self.board_a[line[2]]\n",
    "                and self.board_a[line[2]] == self.board_a[line[3]]\n",
    "                and self.board_a[line[0]] != 0\n",
    "            ):\n",
    "                # 종료됐다면 누가 이겼는지 표시\n",
    "                self.done = True\n",
    "                self.reward = player\n",
    "                return\n",
    "        # 비긴 상태는 더는 보드에 빈 공간이 없을때\n",
    "        observation = self.get_action()\n",
    "        if (len(observation)) == 0:\n",
    "            self.done = True\n",
    "            self.reward = 0\n",
    "        return\n",
    "\n",
    "    # 현재 보드의 상태를 표시 p1 = O, p2 = X\n",
    "    def print_board(self):\n",
    "        print(\"+----+----+----+----+\")\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                if self.board_a[4 * i + j] == 1:\n",
    "                    print(\"|  O\", end=\" \")\n",
    "                elif self.board_a[4 * i + j] == -1:\n",
    "                    print(\"|  X\", end=\" \")\n",
    "                else:\n",
    "                    print(\"|   \", end=\" \")\n",
    "            print(\"|\")\n",
    "            print(\"+----+----+----+----+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Human_player:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.name = \"Human player\"\n",
    "\n",
    "    def select_action(self, env, player):\n",
    "        while True:\n",
    "            # 가능한 행동을 조사한 후 표시\n",
    "            available_action = env.get_action()\n",
    "            print(\"possible actions = {}\".format(available_action))\n",
    "\n",
    "            # 상태 번호 표시\n",
    "            print(\"+----+----+----+----+\")\n",
    "            print(\"+  0 +  1 +  2 +  3 +\")\n",
    "            print(\"+----+----+----+----+\")\n",
    "            print(\"+  4 +  5 +  6 +  7 +\")\n",
    "            print(\"+----+----+----+----+\")\n",
    "            print(\"+  8 +  9 + 10 + 11 +\")\n",
    "            print(\"+----+----+----+----+\")\n",
    "            print(\"+ 12 + 13 + 14 + 15 +\")\n",
    "            print(\"+----+----+----+----+\")\n",
    "\n",
    "            # 키보드로 가능한 행동을 입력 받음\n",
    "            action = input(\"Select action(human) : \")\n",
    "            action = int(action)\n",
    "\n",
    "            # 입력받은 행동이 가능한 행동이면 반복문을 탈출\n",
    "            if action in available_action:\n",
    "                return action\n",
    "            # 아니면 행동 입력을 반복\n",
    "            else:\n",
    "                print(\"You selected wrong action\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 랜덤 플레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random_player:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.name = \"Random player\"\n",
    "        self.print = False\n",
    "\n",
    "    def select_action(self, env, player):\n",
    "        # 가능한 행동 조사\n",
    "        available_action = env.get_action()\n",
    "        # 가능한 행동 중 하나를 무작위로 선택\n",
    "        action = np.random.randint(len(available_action))\n",
    "        #         print(\"Select action(random) = {}\".format(available_action[action]))\n",
    "        return available_action[action]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic 플레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_Critic_player:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.name = \"Actor_Critic player\"\n",
    "        # 상태별 정책(확률 분포)을 저장하는 딕셔너리\n",
    "        self.policy = {}  # 각 상태에서 가능한 행동들의 확률 분포\n",
    "        self.value = {}  # 각 상태의 가치 함수\n",
    "        # 학습률 정의\n",
    "        self.actor_lr = 0.005\n",
    "        self.critic_lr = 0.05\n",
    "        self.gamma = 0.99\n",
    "        self.print = False\n",
    "\n",
    "    # 상태에 맞는 행동을 선택 (액터 부분)\n",
    "    def select_action(self, env, player):\n",
    "        # 정책에 따라 행동을 선택\n",
    "        action = self.policy_action(env)\n",
    "        if self.print:\n",
    "            print(\"{} : select action\".format(action))\n",
    "        return action\n",
    "\n",
    "    def policy_action(self, env):\n",
    "        if self.print:\n",
    "            print(\"-----------   policy_action start -------------\")\n",
    "\n",
    "        # 현재 상태를 키로 변환\n",
    "        state_key = tuple(env.board_a)\n",
    "\n",
    "        # 가능한 행동 목록 조회\n",
    "        available_action = env.get_action()\n",
    "\n",
    "        if self.print:\n",
    "            print(\"{} : available_action\".format(available_action))\n",
    "\n",
    "        # 현재 상태에 대한 행동 확률 분포 초기화 (또는 조회)\n",
    "        if state_key not in self.policy:\n",
    "            # 처음 본 상태라면 균등 확률로 초기화\n",
    "            self.policy[state_key] = {}\n",
    "            for act in available_action:\n",
    "                self.policy[state_key][act] = 1.0 / len(available_action)\n",
    "\n",
    "        if self.print:\n",
    "            print(\"Current state policy: {}\".format(self.policy[state_key]))\n",
    "\n",
    "        # 현재 가능한 행동에 대한 확률만 추출\n",
    "        probs = np.zeros(len(available_action))\n",
    "        for i, act in enumerate(available_action):\n",
    "            if act in self.policy[state_key]:\n",
    "                probs[i] = self.policy[state_key][act]\n",
    "            else:\n",
    "                # 새로운 행동이 추가된 경우 (보드 상태가 변했을 때)\n",
    "                self.policy[state_key][act] = 0.01  # 작은 확률로 초기화\n",
    "                probs[i] = 0.01\n",
    "\n",
    "        # 확률 정규화 (합이 1이 되도록)\n",
    "        probs = probs / np.sum(probs)\n",
    "\n",
    "        if self.print:\n",
    "            print(\"Normalized action probabilities: {}\".format(np.round(probs, 3)))\n",
    "\n",
    "        # 계산된 확률에 따라 행동 선택\n",
    "        action_idx = np.random.choice(range(len(available_action)), p=probs)\n",
    "\n",
    "        if self.print:\n",
    "            print(\"Selected action index: {}\".format(action_idx))\n",
    "            print(\"Selected action: {}\".format(available_action[action_idx]))\n",
    "            print(\"-----------   policy_action end -------------\")\n",
    "\n",
    "        return available_action[action_idx]\n",
    "\n",
    "    # 액터-크리틱 알고리즘으로 학습\n",
    "    def learn(self, state_backup, action_backup, reward, env):\n",
    "        if self.print:\n",
    "            print(\"-----------   learn start -------------\")\n",
    "            print(\n",
    "                \"state_backup = {}, action_backup = {}, reward = {}\".format(\n",
    "                    state_backup, action_backup, reward\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # 상태 키 변환\n",
    "        state_key = tuple(state_backup)\n",
    "\n",
    "        # 현재 상태의 가치를 조회 또는 초기화\n",
    "        if state_key not in self.value:\n",
    "            self.value[state_key] = 0.0\n",
    "\n",
    "        if self.print:\n",
    "            print(\n",
    "                \"Current state value (before update): {}\".format(self.value[state_key])\n",
    "            )\n",
    "\n",
    "        # TD 타깃과 TD 에러 계산\n",
    "        if env.done == True:\n",
    "            # 게임이 끝났을 때의 타깃은 보상 그대로\n",
    "            td_target = reward\n",
    "            if self.print:\n",
    "                print(\"Game ended, TD target = reward: {}\".format(reward))\n",
    "        else:\n",
    "            # 게임이 진행 중일 때는 다음 상태의 가치를 고려\n",
    "            next_state_key = tuple(env.board_a)\n",
    "\n",
    "            # 다음 상태의 가치가 없으면 초기화\n",
    "            if next_state_key not in self.value:\n",
    "                self.value[next_state_key] = 0.0\n",
    "\n",
    "            if self.print:\n",
    "                print(\"Next state value: {}\".format(self.value[next_state_key]))\n",
    "\n",
    "            td_target = reward + self.gamma * self.value[next_state_key]\n",
    "            if self.print:\n",
    "                print(\n",
    "                    \"TD target = reward + gamma * next_state_value: {} + {} * {} = {}\".format(\n",
    "                        reward, self.gamma, self.value[next_state_key], td_target\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # TD 에러 계산\n",
    "        td_error = td_target - self.value[state_key]\n",
    "        if self.print:\n",
    "            print(\n",
    "                \"TD error = TD target - current_state_value: {} - {} = {}\".format(\n",
    "                    td_target, self.value[state_key], td_error\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # 크리틱(가치 함수) 업데이트\n",
    "        old_value = self.value[state_key]\n",
    "        self.value[state_key] += self.critic_lr * td_error\n",
    "        if self.print:\n",
    "            print(\n",
    "                \"Value function update: {} -> {}\".format(\n",
    "                    old_value, self.value[state_key]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # 액터(정책) 업데이트\n",
    "        if state_key not in self.policy:\n",
    "            self.policy[state_key] = {}\n",
    "\n",
    "        if action_backup not in self.policy[state_key]:\n",
    "            self.policy[state_key][action_backup] = 0.01  # 작은 확률로 초기화\n",
    "\n",
    "        if self.print:\n",
    "            print(\n",
    "                \"Current action policy probability (before update): {}\".format(\n",
    "                    self.policy[state_key][action_backup]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # TD 에러를 이용한 정책 업데이트\n",
    "        old_prob = self.policy[state_key][action_backup]\n",
    "        self.policy[state_key][action_backup] += self.actor_lr * td_error\n",
    "\n",
    "        if self.print:\n",
    "            print(\n",
    "                \"Policy probability update: {} -> {}\".format(\n",
    "                    old_prob, self.policy[state_key][action_backup]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # 확률에 음수가 있을 경우 모두 양수가 되도록 보정\n",
    "        min_prob = min(self.policy[state_key].values())\n",
    "        if min_prob < 0:\n",
    "            if self.print:\n",
    "                print(\n",
    "                    \"Negative probability detected: adding {} to all probabilities\".format(\n",
    "                        -min_prob\n",
    "                    )\n",
    "                )\n",
    "            for act in self.policy[state_key]:\n",
    "                self.policy[state_key][act] -= min_prob\n",
    "\n",
    "        # 확률 합이 1이 되도록 정규화\n",
    "        prob_sum = sum(self.policy[state_key].values())\n",
    "        if self.print:\n",
    "            print(\"Pre-normalization probability sum: {}\".format(prob_sum))\n",
    "\n",
    "        for act in self.policy[state_key]:\n",
    "            self.policy[state_key][act] /= prob_sum\n",
    "\n",
    "        if self.print:\n",
    "            print(\n",
    "                \"Normalized policy: {}\".format(\n",
    "                    {a: round(p, 3) for a, p in self.policy[state_key].items()}\n",
    "                )\n",
    "            )\n",
    "            print(\"-----------   learn end -------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic 플레이어 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 762.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1 = 0 p2 = 1 draw = 0\n",
      "학습 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "p1_ACplayer = Actor_Critic_player()\n",
    "p2_ACplayer = Actor_Critic_player()\n",
    "\n",
    "p1_score = 0\n",
    "p2_score = 0\n",
    "draw_score = 0\n",
    "\n",
    "# printer = True\n",
    "print()\n",
    "max_learn = 1\n",
    "\n",
    "for j in tqdm(range(max_learn)):\n",
    "    np.random.seed(j)\n",
    "    env = Environment()\n",
    "\n",
    "    for i in range(20):\n",
    "\n",
    "        # p1 행동 선택\n",
    "        player = 1\n",
    "        pos = p1_ACplayer.select_action(env, player)\n",
    "        # 현재 상태 s, 행동 a를 저장\n",
    "        p1_board_backup = tuple(env.board_a)\n",
    "        p1_action_backup = pos\n",
    "        env.board_a[pos] = player\n",
    "        env.end_check(player)\n",
    "\n",
    "        # 게임이 종료상태라면 각 플레이어 학습\n",
    "        if env.done == True:\n",
    "            # 비겼으면 보수 0\n",
    "            if env.reward == 0:\n",
    "                p1_ACplayer.learn(p1_board_backup, p1_action_backup, 0, env)\n",
    "                if i > 0:  # p2가 한 번이라도 두었을 경우\n",
    "                    p2_ACplayer.learn(p2_board_backup, p2_action_backup, 0, env)\n",
    "                draw_score += 1\n",
    "                break\n",
    "            # p1이 이겼으므로 보상 +1\n",
    "            # p2이 졌으므로 보상 -1\n",
    "            else:  # p1 승리\n",
    "                p1_ACplayer.learn(p1_board_backup, p1_action_backup, 1, env)\n",
    "                if i > 0:  # p2가 한 번이라도 두었을 경우\n",
    "                    p2_ACplayer.learn(p2_board_backup, p2_action_backup, -1, env)\n",
    "                p1_score += 1\n",
    "                break\n",
    "\n",
    "        # 게임이 끝나지 않았다면 p2의 이전 행동에 대해 학습 (게임 시작직후에는 p2는 학습할 수 없음)\n",
    "        if i > 0:\n",
    "            p2_ACplayer.learn(p2_board_backup, p2_action_backup, 0, env)\n",
    "\n",
    "        # p2 행동 선택\n",
    "        player = -1\n",
    "        pos = p2_ACplayer.select_action(env, player)\n",
    "        p2_board_backup = tuple(env.board_a)\n",
    "        p2_action_backup = pos\n",
    "        env.board_a[pos] = player\n",
    "        env.end_check(player)\n",
    "\n",
    "        if env.done == True:\n",
    "            # 비겼으면 보수 0\n",
    "            if env.reward == 0:\n",
    "                p1_ACplayer.learn(p1_board_backup, p1_action_backup, 0, env)\n",
    "                p2_ACplayer.learn(p2_board_backup, p2_action_backup, 0, env)\n",
    "                draw_score += 1\n",
    "                break\n",
    "            # p2이 이겼으므로 보상 +1\n",
    "            # p1이 졌으므로 보상 -1\n",
    "            else:\n",
    "                p1_ACplayer.learn(p1_board_backup, p1_action_backup, -1, env)\n",
    "                p2_ACplayer.learn(p2_board_backup, p2_action_backup, 1, env)\n",
    "                p2_score += 1\n",
    "                break\n",
    "\n",
    "        # 게임이 끝나지 않았다면 p1의 행동에 대해 학습\n",
    "        p1_ACplayer.learn(p1_board_backup, p1_action_backup, 0, env)\n",
    "\n",
    "    # # 1000 게임마다 게임 결과 표시\n",
    "    # if j % 1000 == 0:\n",
    "    #     print(\n",
    "    #         \"j = {} p1 = {} p2 = {} draw = {}\".format(j, p1_score, p2_score, draw_score)\n",
    "    #     )\n",
    "\n",
    "print(\"p1 = {} p2 = {} draw = {}\".format(p1_score, p2_score, draw_score))\n",
    "print(\"학습 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 게임 진행 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pl player : Human player\n",
      "p2 player : Actor_Critic player\n",
      "possible actions = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "+----+----+----+----+\n",
      "+  0 +  1 +  2 +  3 +\n",
      "+----+----+----+----+\n",
      "+  4 +  5 +  6 +  7 +\n",
      "+----+----+----+----+\n",
      "+  8 +  9 + 10 + 11 +\n",
      "+----+----+----+----+\n",
      "+ 12 + 13 + 14 + 15 +\n",
      "+----+----+----+----+\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# p1 = Human_player()\n",
    "p2 = Human_player()\n",
    "\n",
    "# p1 = Random_player()\n",
    "# p2 = Random_player()\n",
    "\n",
    "# p1 = Monte_Carlo_player()\n",
    "# p1.num_playout = 100\n",
    "# p2 = Monte_Carlo_player()\n",
    "# p2.num_playout = 1000\n",
    "\n",
    "# p1 = p1_Qplayer\n",
    "# p1.epsilon = 0\n",
    "\n",
    "# p2 = p2_Qplayer\n",
    "# p2.epsilon = 0\n",
    "\n",
    "p1 = Actor_Critic_player()\n",
    "# p2 = Actor_Critic_player()\n",
    "\n",
    "# p1 = p1_DQN\n",
    "# p1.epsilon = 0\n",
    "\n",
    "# 지정된 게임 수를 자동으로 두게 할 것인지 한게임씩 두게 할 것인지 결정\n",
    "# auto = True : 지정된 판수(games)를 자동으로 진행\n",
    "# auto = False : 한판씩 진행\n",
    "\n",
    "auto = False\n",
    "\n",
    "# auto 모드의 게임수\n",
    "games = 100\n",
    "\n",
    "print(\"pl player : {}\".format(p1.name))\n",
    "print(\"p2 player : {}\".format(p2.name))\n",
    "\n",
    "# 각 플레이어의 승리 횟수를 저장\n",
    "p1_score = 0\n",
    "p2_score = 0\n",
    "draw_score = 0\n",
    "\n",
    "if auto:\n",
    "    # 자동 모드 실행\n",
    "    for j in tqdm(range(games)):\n",
    "\n",
    "        np.random.seed(j)\n",
    "        env = Environment()\n",
    "\n",
    "        for i in range(10000):\n",
    "            # p1 과 p2가 번갈아 가면서 게임을 진행\n",
    "            # p1(1) -> p2(-1) -> p1(1) -> p2(-1) ...\n",
    "            reward, done = env.move(p1, p2, (-1) ** i)\n",
    "            # 게임 종료 체크\n",
    "            if done == True:\n",
    "                if reward == 1:\n",
    "                    p1_score += 1\n",
    "                elif reward == -1:\n",
    "                    p2_score += 1\n",
    "                else:\n",
    "                    draw_score += 1\n",
    "                break\n",
    "\n",
    "else:\n",
    "    # 한 게임씩 진행하는 수동 모드\n",
    "    np.random.seed(1)\n",
    "    while True:\n",
    "\n",
    "        env = Environment()\n",
    "        env.print = False\n",
    "        for i in range(10000):\n",
    "            reward, done = env.move(p1, p2, (-1) ** i)\n",
    "            env.print_board()\n",
    "            if done == True:\n",
    "                if reward == 1:\n",
    "                    print(\"winner is p1({})\".format(p1.name))\n",
    "                    p1_score += 1\n",
    "                elif reward == -1:\n",
    "                    print(\"winner is p2({})\".format(p2.name))\n",
    "                    p2_score += 1\n",
    "                else:\n",
    "                    print(\"draw\")\n",
    "                    draw_score += 1\n",
    "                break\n",
    "\n",
    "        # 최종 결과 출력\n",
    "        print(\"final result\")\n",
    "        env.print_board()\n",
    "\n",
    "        # 한게임 더?최종 결과 출력\n",
    "        answer = input(\"More Game? (y/n)\")\n",
    "\n",
    "        if answer == \"n\":\n",
    "            break\n",
    "\n",
    "print(\n",
    "    \"p1({}) = {} p2({}) = {} draw = {}\".format(\n",
    "        p1.name, p1_score, p2.name, p2_score, draw_score\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
